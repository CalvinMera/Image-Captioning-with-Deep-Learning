{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.8.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.8 64-bit ('mlp_1_env': conda)"
    },
    "interpreter": {
      "hash": "56391efcfd407cdfb7177f7052d9ec0dabb31d2434d8abe883c176e68133b969"
    },
    "colab": {
      "name": "preprocess_model_train_evaluate.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmcCRN_getXR"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qlb2_uyCfOF0"
      },
      "source": [
        "# We uploaded our Jupyter notebook and data (images, cached features, and captions) onto Google Colaboratory.\n",
        "# There are a few things that must be taken care of.\n",
        "# First, we must unzip the folder containing our images.\n",
        "# In order to reduce the run time, we use the validation set as our training set.\n",
        "# !unzip '/content/drive/MyDrive/Image_Captioning/val.zip' -d '/content/drive/My Drive/Image_Captioning/data'\n",
        "\n",
        "# Note that you will have to switch out these 2 ending paths: /MyDrive/Image_Captioning/val.zip and /MyDrive/Image_Captioning/data\n",
        "# in order to accomodate the coordinates of your files."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K506oC21eeR0"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3mKam_huyH-"
      },
      "source": [
        "# We change our current directory, so that (os.path.abspath(.) ...) generates the correct path to our dataset.\n",
        "%cd /content/drive/MyDrive/Image_Captioning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TvLmRz2eYh7"
      },
      "source": [
        "# Listing the paths and filenames of interest.\n",
        "annotations_path = {'val':'/val_cleaned.json'}\n",
        "images_path = {'val':'/data/val/'}\n",
        "\n",
        "# The role of captions_and_images is to generate a 2-tuple, where\n",
        "# the first entry houses the captions, and\n",
        "# the second entry houses the associated image paths.\n",
        "\n",
        "# We shall do this again once we have processed the images according to the requirements of ResNet-50 and\n",
        "# once we have tokenized and padded our captions.\n",
        "\n",
        "# As of now, the validation set contains 6,982 images and 32,299 captions.\n",
        "# We take 80% of those examples for the training phase, and the rest for the validation phase.\n",
        "# Note that we did train on the entire training dataset (21,220 images and 98,015 captions) for a single epoch.\n",
        "# This took almost 4.5 hours to complete\n",
        "\n",
        "# We will uncover shortly that the function below must be slightly altered in order\n",
        "# to accomodate the nuances featured in our dataset.\n",
        "\n",
        "def captions_and_images_testing (dataset):\n",
        "\n",
        "# First, we open the dataset, which is saved as a .json file.\n",
        "    with open(os.path.abspath('.') + annotations_path[dataset], 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "# Translating the .json file from above to a Pandas data frame.\n",
        "    df = pd.DataFrame.from_dict(data, orient='index')\n",
        "# Before we can iterate over the index of the data frame, we must make sure that the indices of neighboring entries differ by 1.\n",
        "# Note that neighboring indices do not always differ by 1 because we have removed images from our dataset that did not have enough\n",
        "# captions. Initially, all images had 5 captions, but we deleted any captions that had the options is_precanned or is_rejected set\n",
        "# equal to True. We then proceeded to drop any images with 2 or less captions, i.e., we kept any image with at least 3 nontrivial captions.\n",
        "    df = df.reset_index()\n",
        "# Resetting the index creates a column that houses the previous indices. We drop this column.\n",
        "    df = df.drop(['index'], axis = 1)\n",
        "\n",
        "# We collect the image path and caption of every row.\n",
        "# Then, we form a dictionary, whose keys are the image paths and whose values are the associated captions.\n",
        "    image_path_to_caption = collections.defaultdict(list)\n",
        "    for n in range(df.shape[0]):\n",
        "        image_path = os.path.abspath('.') + images_path[dataset] + df.file_name.iloc[n]\n",
        "        caption = f'<start> {df.caption.iloc[n]} <end>' # The curly brackets ensure that output is generated into text.\n",
        "        image_path_to_caption[image_path].append(caption)\n",
        "\n",
        "# We now separate the captions and image paths into 2 lists.\n",
        "    captions = [] \n",
        "    img_name_vector = []\n",
        "\n",
        "    for image_path in image_path_to_caption.keys(): # The keys are unique, so we only see each image path once.\n",
        "        caption_list = image_path_to_caption[image_path] # Each image has at least 3 captions.\n",
        "        captions.extend(caption_list) # We add the captions to caption_list.\n",
        "        img_name_vector.extend([image_path]*len(caption_list)) # We add the image path multiple times to reflect that images have\n",
        "        # multiple captions.\n",
        "    return captions, img_name_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqBLZVNX5ezA"
      },
      "source": [
        "# Before we proceed, we collect the captions according to their length as there might be something fishy.\n",
        "captions_and_images_explore = captions_and_images_testing('val')\n",
        "sizes_to_indices = collections.defaultdict(list)\n",
        "for index, caption in enumerate(captions_and_images_explore[0]):\n",
        "  size = len(caption)\n",
        "  sizes_to_indices[size].append(index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1O7nnEiDMsy"
      },
      "source": [
        "# We check that the validation set has 32,299 captions.\n",
        "total = 0\n",
        "for key in sizes_to_indices.keys():\n",
        "    total += len(sizes_to_indices[key])\n",
        "print(total==32299)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDLoVAEg6jAQ"
      },
      "source": [
        "descending_sizes = sorted(list(sizes_to_indices.keys()),reverse=True)\n",
        "descending_sizes[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R5E3HQgi98j"
      },
      "source": [
        "# We now explore what a caption of length 874 looks like.\n",
        "captions_and_images_explore[0][sizes_to_indices[874][0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJBZcuys8Sza"
      },
      "source": [
        "zeroth_caption = captions_and_images_explore[0][0]\n",
        "print(f'This is caption of size {len(zeroth_caption)}: {zeroth_caption}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN82xZrWnDpu"
      },
      "source": [
        "# Observe that the caption of length 874 is incredibly long. Meanwhile, a caption of length 68 contains 10 words\n",
        "# and the start and end token. Let's see what a caption of length 200 looks like.\n",
        "captions_and_images_explore[0][sizes_to_indices[258][0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu6Fyjmf9E0V"
      },
      "source": [
        "# The above caption seems reasonable and relevant, so we shall impose a cutoff of length 300.\n",
        "# Imposing the cutoff should generate a collection of captions, where the maximum possible length of a caption is 294.\n",
        "# Here, one could impose various cutoffs and study how our model performs. We do not pursue this avenue.\n",
        "\n",
        "def captions_and_images (dataset):\n",
        "\n",
        "    with open(os.path.abspath('.') + annotations_path[dataset], 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "\n",
        "    df = pd.DataFrame.from_dict(data, orient='index')\n",
        "    df = df.reset_index()\n",
        "    df = df.drop(['index'], axis = 1)\n",
        "\n",
        "    image_path_to_caption = collections.defaultdict(list)\n",
        "    for n in range(df.shape[0]):\n",
        "        image_path = os.path.abspath('.') + images_path[dataset] + df.file_name.iloc[n]\n",
        "        caption = f'<start> {df.caption.iloc[n]} <end>'\n",
        "        # Here, we use an if-continue statement that will ignore captions of length greater than 300.\n",
        "        if len(caption) > 300:\n",
        "          continue\n",
        "        image_path_to_caption[image_path].append(caption)\n",
        "\n",
        "    captions = [] \n",
        "    img_name_vector = []\n",
        "\n",
        "    for image_path in image_path_to_caption.keys():\n",
        "        caption_list = image_path_to_caption[image_path]\n",
        "        # We have removed some captions, so we might have images with 2 or less captions.\n",
        "        # We dispose of them with another if-continue statement.\n",
        "        if len(caption_list) < 3:\n",
        "          continue\n",
        "        captions.extend(caption_list)\n",
        "        img_name_vector.extend([image_path]*len(caption_list))\n",
        "    return captions, img_name_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w60PczdS-S2w"
      },
      "source": [
        "captions_and_images_validation = captions_and_images('val')\n",
        "sizes_to_indices_again = collections.defaultdict(list)\n",
        "for index, caption in enumerate(captions_and_images_validation[0]):\n",
        "  size = len(caption)\n",
        "  sizes_to_indices_again[size].append(index)\n",
        "sorted(list(sizes_to_indices_again.keys()),reverse=True)[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPnIeGkYDEE4"
      },
      "source": [
        "# We check how many captions are left in the validation set.\n",
        "total_again =  0\n",
        "for key in sizes_to_indices_again.keys():\n",
        "    total_again += len(sizes_to_indices_again[key])\n",
        "print(total_again)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMIj58Q6eYh9"
      },
      "source": [
        "# The function below will output a preprocessed image ready for ResNet-50 and its path.\n",
        "def load_image (image_path):\n",
        "    image = tf.io.read_file(image_path) # Loads the raw data as a string\n",
        "    image = tf.image.decode_jpeg(image, channels = 3) # Decodes JPEG-encoded image to a uint8 (8-bit unsigned integer) tensor\n",
        "    image = tf.image.resize(image, (224,224)) # Resizes the tensor above to agree with the input size of ResNet-50\n",
        "    image = tf.keras.applications.resnet50.preprocess_input(image) # Takes care of other ResNet-50 preprocessing steps.\n",
        "    return image, image_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEBPsbxHeYh_"
      },
      "source": [
        "# The output of load_image is:\n",
        "# tf.Tensor of shape == (224, 224, 3), whose entries are float32 and the image path.\n",
        "# The first index accesses the img_name_vector entry,\n",
        "# the second index acceses the 0th file path, and\n",
        "# the third index accesses the processed image as a tensor or its image path.\n",
        "# load_image(captions_and_images_validation[1][0])[0]\n",
        "# load_image(captions_and_images_validation[1][0])[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nunt0rL-eYh_"
      },
      "source": [
        "# # We comment out this cell because we have already cached the features that was generated by ResNet-50.\n",
        "# # We now create a model that is ResNet-50 minus the last layer.\n",
        "# # After the images have run its course through the model, we store the output as a vector, which contains all the extracted features.\n",
        "# # We save these extracted features to disk.\n",
        "\n",
        "# def cache_extracted_features (dataset):\n",
        "\n",
        "# # We instantiate the ResNet-50 model without the top layer and load the ImageNet weights.\n",
        "# # Setting include_top to False results in dropping these 2 layers: avg_pool (GlobalAveragePooling2) and predictions (Dense).\n",
        "#     image_model = tf.keras.applications.resnet50.ResNet50(include_top=False, weights='imagenet')\n",
        "\n",
        "#     new_input = image_model.input # We grab the input of the model.\n",
        "#     hidden_layer = image_model.layers[-1].output # We grab everything but the last layers.\n",
        "\n",
        "# # We build a model using the functional API, where input and output are defined above.\n",
        "#     image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
        "\n",
        "# # We extract only the unique paths and then sort them.\n",
        "#     encode_data = sorted(set(img_name_vector))\n",
        "\n",
        "# # We use the data API to generate a sequence of data items.\n",
        "# # Note that as of now the image_dataset is a sequence of tensors that contain strings reflecting our file paths.\n",
        "#     image_dataset = tf.data.Dataset.from_tensor_slices(encode_data)\n",
        "\n",
        "# # We can take a peak of the first element of this dataset by running the following code:\n",
        "# # for item in image_dataset.take(1):\n",
        "# #     print(item)\n",
        "# # The output for the training data set is:\n",
        "# # tf.Tensor(b'/Users/calvin/projects/mlp_1/data/images/train/VizWiz_train_00000000.jpg', shape=(), dtype=string)\n",
        "\n",
        "# # We use the map method to apply the load_image function to our dataset.\n",
        "\n",
        "# # tf.data.AUTOTUNE ensures that the number of batches that are computed in parallel is based on available sources.\n",
        "# # The batch method sets the number of images that are dealt with in a single batch.\n",
        "#     image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n",
        "\n",
        "# # After applying the load_image function to our dataset, we obtain images that have been processed,\n",
        "# # i.e., they have been read, decoded, resized, and preprocessed according to the requirements of ResNet-50.\n",
        "\n",
        "# # As before, we can see the output of this newly transformed image_dataset with the following code:\n",
        "# # for item in image_dataset.take(1):\n",
        "# #    print(item)\n",
        "# # The output is a tf.Tensor of shape == (batch_size, 224, 224, 3) with (batch_size) image paths.\n",
        "\n",
        "# # Now that we have a sequence of data items, specifically, a preprocessed image and its associated file path we perform\n",
        "# # the following for loop.\n",
        "\n",
        "# # First, we extract the features of our image. Then, we resize the extracted features.\n",
        "#     for img, path in tqdm(image_dataset):\n",
        "#         batch_features = image_features_extract_model(img) \n",
        "#         batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3])) \n",
        "\n",
        "# # We perform another for loop, but this time it is not over the 2-tuple, (image, path) it is over the 2-tuple,\n",
        "# # (extracted features, paths). Lastly, we save the path to a binary file using the file format, (...).npy.\n",
        "#         for bf, p in zip(batch_features, path):\n",
        "#             path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "#             np.save(path_of_feature, bf.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yz2Df4KveYiB"
      },
      "source": [
        "# We use list comprehension to generate the lengths of all the elements of the input, which we call tensor.\n",
        "# Then, we return the element with the longest length.\n",
        "# Note that writing max([...]) is valid too, but one can get away with leaving out the square brackets.\n",
        "def calc_max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-m5d6x9eYiC"
      },
      "source": [
        "# We tokenize and pad our captions.\n",
        "def tokenize_pad (captions):\n",
        "    \n",
        "    top_k = 5000 # We set a limit to the number of words we hold in our vocabulary.\n",
        "\n",
        "# num_words: determines the maximum number of words to keep, i.e., only the (num_words - 1) most common words are kept.\n",
        "# oov_token: if we encounter a word that is not part of our vocabulary, then we assign it the special token, <unk>.\n",
        "# filters: a string of characters that are filtered from the text being analyzed.\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k, oov_token=\"<unk>\", filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~')\n",
        "\n",
        "# We update our internal vocabulary based on the captions we extracted from the data frame above.\n",
        "# Note that we feed a list of captions to our model, not a sequence, so we use fit_on_texts rather than fit_on_sequences.\n",
        "    tokenizer.fit_on_texts(captions)\n",
        "\n",
        "# Creating a word to index mapping and a index to word mapping.\n",
        "# We reserve the index, 0 for the special token, <pad>.\n",
        "    tokenizer.word_index['<pad>'] = 0\n",
        "    tokenizer.index_word[0] = '<pad>'\n",
        "\n",
        "# Transforming our collection of texts into a sequence of integers.\n",
        "# As we mentioned above, only the (num_words - 1) most frequent word are taken into account.\n",
        "    seqs = tokenizer.texts_to_sequences(captions)\n",
        "    max_length = calc_max_length(seqs)\n",
        "\n",
        "# Note that we can check out the inner workings of our tokenizer with the following methods and attributes of Tokenizer:\n",
        "# get_config(), index_word, word_index.\n",
        "\n",
        "# Note that we did not lower the characters of our string because this is taken care by the default settings of\n",
        "# tf.keras.preprocessing.text.Tokenizer.\n",
        "\n",
        "# Not all sequences share the same length, so we first find the longest sequence, and then we add 0's to all sequences shorter than\n",
        "# the longest sequence.\n",
        "    cap_vector = tf.keras.preprocessing.sequence.pad_sequences(seqs, padding='post') \n",
        "\n",
        "    return cap_vector, tokenizer, seqs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-jVuAHFeYiC"
      },
      "source": [
        "# We make sure that the above generates a functioning tokenizer.\n",
        "# Note that we will be using the tokenizer during the training phase, which is why we altered the output.\n",
        "# We also returns seqs in order to appreciate some of the methods and attributes of our tokenizer.\n",
        "tokenizer_val= tokenize_pad(captions_and_images_validation[0])\n",
        "# We print the first 8 indices and their associated words.\n",
        "for n in range(8):\n",
        "  print(n, tokenizer_val[1].index_word[n])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNz2T7GLeYiD"
      },
      "source": [
        "zeroth_caption = captions_and_images_validation[0][0] # grabbing the 0th caption\n",
        "print(zeroth_caption) # generating the 0th caption\n",
        "# We can recover the indices needed to generate the words from the tokenizer by accessing the 0th entry of seqs.\n",
        "print([tokenizer_val[1].index_word[n] for n in tokenizer_val[2][0]]) # generating the 0th caption for seqs (tokenized)\n",
        "tokenized_padded_test = [tokenizer_val[1].index_word[n] for n in tokenizer_val[0][0]]\n",
        "print(tokenized_padded_test) # generating the 0th caption from cap_vector (tokenized and padded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7oT2Qp_eYiE"
      },
      "source": [
        "# We now check the maximum length of our tokenized/padded captions.\n",
        "print(len(tokenizer_val[0][0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TQyikD7eYiF"
      },
      "source": [
        "# We list the parameters, which we will be using later on.\n",
        "\n",
        "# BATCH_SIZE = 64\n",
        "# BUFFER_SIZE = 1000 or buffer_size=tf.data.AUTOTUNE\n",
        "\n",
        "# embedding_dim = 256 (used in the encoder and decoder models)\n",
        "# units = 512 (used in the decoder and attention models)\n",
        "# vocab_size = top_k + 1 (used in the decoder model)\n",
        "\n",
        "# num_steps equals the number of captions in our dataset divided (integer or floor) by the batch size.\n",
        "# The number of captions is 98,015 and 32,299 for the training and validation dataset, respectively.\n",
        "# These values can be found in the notebook, 3_preprocessing_train_val.ipynb.\n",
        "# These numbers only hold true if we do not remove very long captions like we did above.\n",
        "\n",
        "# num_steps = len(pro_img_and_tok_cap(dataset)[0]) // BATCH_SIZE\n",
        "\n",
        "# Shape of the vector extracted from InceptionV3 is (64, 2048).\n",
        "# Shape of the vector extracted from ResNet-50 is (49, 2048).\n",
        "\n",
        "# features_shape = 2048\n",
        "# attention_features_shape = 49\n",
        "\n",
        "# How can we recover these numbers?\n",
        "# We generate a summary of the ResNet-50 model using the .summary() method and \n",
        "# see the output shape of the layer named conv5_block3_out. It is (None, 7, 7, 2048).\n",
        "# Repeating this for InceptionV3, we recover a shape of (None, 8, 8,8 2048).\n",
        "\n",
        "# tf.keras.layers.Layer(tf.keras.applications.resnet50.ResNet50(include_top=True, weights='imagenet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0g6Uary2eYiE"
      },
      "source": [
        "# We now generate a 2-tuple, where\n",
        "# the first entry houses the image paths to the preprocessed images and\n",
        "# the second entry houses the associated tokenized and padded captions.\n",
        "\n",
        "def pro_img_and_tok_cap (dataset):\n",
        "\n",
        "# Just as before with captions_and_images, the pattern we follow is:\n",
        "# Generate a dictionary, where the keys are the images and the values are all of its associated captions.\n",
        "# Then, generate 2 lists, where the first holds the captions and the second holds the images (multiplicity being taken into account).\n",
        "    img_to_cap_vector = collections.defaultdict(list) # keys are image paths, values are tokenized/padded captions.\n",
        "    \n",
        "# Recall that the output of captions_and_images(dataset) is (captions, img_name_vector).\n",
        "# Recall that the output of tokenize_pad(captions) is (cap_vector, tokenizer, seqs).\n",
        "\n",
        "    captions_and_images_data = captions_and_images(dataset)\n",
        "\n",
        "    img_name_vector = captions_and_images_data[1]\n",
        "    cap_vector = tokenize_pad(captions_and_images_data[0])[0]\n",
        "\n",
        "    for img, cap in zip(img_name_vector, cap_vector):\n",
        "        img_to_cap_vector[img].append(cap)\n",
        "\n",
        "    # We now create the training and validation split.\n",
        "    img_keys = list(img_to_cap_vector.keys()) # First, we generate the unique keys, i.e., the unique image paths.\n",
        "    # Shuffling all the keys. Note that we pick a seed in order to replicate our results.\n",
        "    random.seed(1729)\n",
        "    random.shuffle(img_keys)\n",
        "    # We create an indexing scheme that separates the first 80 percent of the recently shuffled dataset.\n",
        "    slice_index = int(len(img_keys)*0.8) \n",
        "    \n",
        "    img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n",
        "    \n",
        "    # We now create 2 lists per dataset (training and validation), which houses the images and captions.\n",
        "\n",
        "    img_name_train = []\n",
        "    cap_train = []\n",
        "\n",
        "    for imgt in img_name_train_keys:\n",
        "        capt_len = len(img_to_cap_vector[imgt])\n",
        "        img_name_train.extend([imgt] * capt_len)\n",
        "        cap_train.extend(img_to_cap_vector[imgt])\n",
        "\n",
        "    img_name_val = []\n",
        "    cap_val = []\n",
        "\n",
        "    for imgv in img_name_val_keys:\n",
        "        capv_len = len(img_to_cap_vector[imgv])\n",
        "        img_name_val.extend([imgv] * capv_len)\n",
        "        cap_val.extend(img_to_cap_vector[imgv])\n",
        "    \n",
        "    return img_name_train, cap_train, img_name_val, cap_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIoLWzm-eYiG"
      },
      "source": [
        "# The function below is used to load the numpy files, which house the extracted features generated by ResNet-50.\n",
        "# Note that the output is the extracted features of the processed image and its associated tokenized/padded caption.\n",
        "def map_func(img_name, cap):\n",
        "  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
        "  return img_tensor, cap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFADnH0feYiG"
      },
      "source": [
        "# We now create a tf.data.Dataset dataset which will be fed into our models.\n",
        "\n",
        "# We added an option to control the batch size in order to easily generate the dimensionality of the variables in question.\n",
        "# We set batch_size to a default value of 64.\n",
        "\n",
        "# Instead of generating solely a dataset, we generate a 2-tuple where the 0th entry is the dataset, but\n",
        "# the 1st entry is the processed images along with their associated tokenized/padded captions.\n",
        "\n",
        "# We emphasize that these extra inputs and outputs are to explore the dimensionality of what we are creating.\n",
        "\n",
        "# Furthermore, since we have performed a training-validation split earlier in the function, pro_img_and_tok_cap, we wish \n",
        "# to have the control to work with the training images/captions or the validation images/captions later on.\n",
        "\n",
        "def create_dataset (data, batch_size=64):\n",
        "    # Creating a sequence of data items.\n",
        "    _pro_img_and_tok_cap = pro_img_and_tok_cap(data)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((_pro_img_and_tok_cap[0], _pro_img_and_tok_cap[1])) # single positional argument!\n",
        "\n",
        "    # Use map to load the numpy files holding the cached features in parallel.\n",
        "    # We use tf.numpy_function to promote the Python function to a TensorFlow operation.\n",
        "    dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
        "            map_func, [item1, item2], [tf.float32, tf.int32]),\n",
        "            num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    # Shuffling and creating batches.\n",
        "    dataset = dataset.shuffle(1000).batch(batch_size) \n",
        "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset, _pro_img_and_tok_cap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PH03ye9YeYiH"
      },
      "source": [
        "# We now investigate the shape of the outputs of the function, create_dataset.\n",
        "# We do this for the validation dataset with a batch size equal to 2 in order to shorten the run time.\n",
        "dataset_testing = create_dataset('val', 1)[0]\n",
        "# for item in dataset_testing.take(1):\n",
        "#      print(item)\n",
        "# The outputs are as follows:\n",
        "# The first entry is the processed image: tf.Tensor of shape: (batch_size = 2, 49, 2048), dtype = float32\n",
        "# The second entry is the tokenized/padded caption: tf.Tensor of shape: (batch_size = 2 , 61), dtype = int32\n",
        "# Note that the 61 is the longest tokenized/padded caption in our dataset."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psPC_bWTeYiH"
      },
      "source": [
        "# We also investigate the shape of several variables that will play a crucial role in what follows.\n",
        "count = 0\n",
        "for (batch, (img_tensor, target)) in enumerate(dataset_testing):\n",
        "  if count == 1:\n",
        "    break\n",
        "  print(f'This is the nth batch with n equal to {batch}.')\n",
        "  print(f'The shape of img_tensor is {img_tensor.shape}.')\n",
        "  print(f'The shape of target is {target.shape}.')\n",
        "  print(' ')\n",
        "  print('Below is the sequence representation of target.')\n",
        "  print(target.numpy())\n",
        "  print(' ')\n",
        "  print('Below is sequence representation of target as a tokenized/padded caption.')\n",
        "  print([tokenizer_val[1].index_word[n] for n in target[0].numpy()])\n",
        "  # For the evaluation phase, the target.shape[0]=1 because there is only a single image we must generate a caption for.\n",
        "  # For the training phase, the target.shape[0]=batch_size because we are training multiple examples simulatenously.\n",
        "  print(' ')\n",
        "  initial_decoder_input = tf.expand_dims([tokenizer_val[1].word_index['<start>']]* target.shape[0], 1)\n",
        "  print(f'This is the initial decoder input: {initial_decoder_input}. The shape of the initial decoder input is {initial_decoder_input.shape}.')\n",
        "  print(' ')\n",
        "  empty_list = []\n",
        "  print('Below we generate the tokenized/padded form of the contents of target.')\n",
        "  for i in range(11):\n",
        "    empty_list.append([tokenizer_val[1].index_word[n] for n in target[:,i].numpy()][0])\n",
        "  print(empty_list)\n",
        "  print(' ')\n",
        "  print(f'The shape of the 7th decoder input is {tf.expand_dims(target[:,7],1).shape}.')\n",
        "  for i in range(11):\n",
        "    ith_decoder_input = tf.expand_dims(target[:, i], 1)\n",
        "    print(ith_decoder_input, [tokenizer_val[1].index_word[n] for n in ith_decoder_input[0].numpy()])\n",
        "  count += 1\n",
        "  print(' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGzX9s-QeYiI"
      },
      "source": [
        "# We are now ready to tackle our encoder-decoder model.\n",
        "# Constructing the encoder model.\n",
        "\n",
        "class CNN_Encoder(tf.keras.Model):\n",
        "\n",
        "    # We are subclassing tf.keras.Model, so we define our layers in __init__ and\n",
        "    # any calculations this model carries out in the call function.\n",
        "    # Those operations can be executed on some input, x by running CNN_Encoder(embedding_dim)(x).\n",
        "    # The syntax has changed in Python 3.0, so we can replace super(CNN_Encoder, self).__init__() with super().__init__()\n",
        "    \n",
        "    def __init__(self, embedding_dim):\n",
        "\n",
        "      super().__init__()\n",
        "      \n",
        "      self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x, print_stuff):\n",
        "      # Recall that if the input to a dense layer has a rank of at least 3, then passing \n",
        "      # through a dense layer with (units) neurons yields the following mapping of dimensions:\n",
        "      # input: (..., n) -> output shape: (..., units)\n",
        "\n",
        "        if print_stuff:\n",
        "            print(f'Shape of img_tensor before dense layer and ReLU (CNN): {x.shape}.')\n",
        "        # shape before dense layer: (batch_size, 49, 2048)\n",
        "\n",
        "        x = self.fc(x)\n",
        "\n",
        "        # shape after dense layer: (batch_size, 49, embedding_dim)\n",
        "\n",
        "        x = tf.nn.relu(x)\n",
        "\n",
        "        # shape after ReLU: (batch_size, 49, embedding_dim)\n",
        "\n",
        "        if print_stuff:\n",
        "            print(f'Shape of img_tensor after dense layer and ReLU (CNN): {x.shape}.')\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOXo3gTPeYiI"
      },
      "source": [
        "# Investigating the shape of the img_tensor before and after it passes through the encoder.\n",
        "for (batch, (img_tensor, target)) in enumerate(dataset_testing):\n",
        "     x = CNN_Encoder(256)(img_tensor, True)\n",
        "     break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXrR6_2neYiJ"
      },
      "source": [
        "# Constructing the attention model.\n",
        "\n",
        "class BahdanauAttention(tf.keras.Model):\n",
        "\n",
        "   # The purpose of the attention model is to have the decoder focus on certain portions of\n",
        "   # the encoder output at each time step.\n",
        "   # What we implement is not purely Bahdanau attention.\n",
        "   # It is instead a blend of Bahdanau attention and Luong attention.\n",
        "   # Bahdanau: we concatenate the decoder hidden state with the encoder output to generate scores.\n",
        "   # Bahdanau: we use the previous hidden state to generate scores\n",
        "   # Luong: we use output of the attention model as an input to the decoder.\n",
        "\n",
        "  def __init__(self, units):\n",
        "    super().__init__()\n",
        "    # We have 2 dense layers each with (units) neurons.\n",
        "    # One layer is for the cached features the CNN encoder generates.\n",
        "    # The other layer is for the hidden state of the RNN decoder.\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    # We have a layer with a single neuron in order to generate a number, not a tensor\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "    \n",
        "  def call(self, features, hidden, print_stuff):\n",
        "   \n",
        "    if print_stuff:\n",
        "      print(f'Shape of hidden before tf.expand_dims (Bahdanau): {hidden.shape}.')\n",
        "    # hidden shape: (batch_size, units)\n",
        "\n",
        "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "    if print_stuff:\n",
        "      print(f'Shape of hidden after tf.expand_dims (Bahdanau): {hidden_with_time_axis.shape}.')\n",
        "    # hidden_with_time_axis shape: (batch_size, 1, units)\n",
        "\n",
        "    if print_stuff:\n",
        "      print(f'Shape of features before W1 (Bahdanau): {features.shape}.')\n",
        "    # features before W1: (batch_size, 49, embedding_dim)\n",
        "\n",
        "    W1_product = self.W1(features)\n",
        "\n",
        "    if print_stuff:\n",
        "      print(f'Shape of features after W1 (Bahdanau): {W1_product.shape}.')\n",
        "    # features after W1: (batch_size, 49, units)\n",
        "\n",
        "    if print_stuff:\n",
        "      print(f'Shape of hidden_with_time_axis before W2 (Bahdanau): {hidden_with_time_axis.shape}.')\n",
        "    # hidden_with_time_axis before W2: (batch_size, 1, units)\n",
        "\n",
        "    W2_product = self.W2(hidden_with_time_axis)\n",
        "\n",
        "    if print_stuff:\n",
        "      print(f'Shape of hidden_with_time_axis after W2 (Bahdanau): {W2_product.shape}.')\n",
        "    # hidden_with_time_axis after W2: (batch_size, 1, units)\n",
        "\n",
        "    # Note that we are adding two objects whose shapes are:\n",
        "    # (batch_size, 49, units) and (batch_size, 1, units).\n",
        "\n",
        "    # At first glance this algebraic operation might seem incompatible.\n",
        "    # Recall we have added/multiplied numbers of different dimensions.\n",
        "    # As an example, think about the shapes involved with scalar multiplication.\n",
        "    # Here, we multiply a single number with an array of numbers.\n",
        "\n",
        "    # The rule that TensorFlow follows is called broadcasting.\n",
        "    # First, start at the rightmost entry, and check if they are either equal or 1.\n",
        "    # If either of those conditons are met, then the resulting dimension is the largest one\n",
        "    # Obviosuly, if they are the same, then they are both the largest ones.\n",
        "    # Proceed to the left entry and repeat.\n",
        "\n",
        "    attention_hidden_layer = (tf.math.tanh(W1_product + W2_product))\n",
        "\n",
        "    if print_stuff:\n",
        "      print(f'To obtain the shape of the attention_hidden_layer, we add 2 tensors (Bahdanau).')\n",
        "      print(f'Their shapes are (Bahdanau): {W1_product.shape} and {W2_product.shape}.')\n",
        "      print(f'Shape of attention_hidden_layer (Bahdanau): {attention_hidden_layer.shape}.')\n",
        "    # attention_hidden_layer shape: (batch_size, 49, units)\n",
        "\n",
        "    score = self.V(attention_hidden_layer)\n",
        "\n",
        "    if print_stuff:\n",
        "      print(f'Shape of score (Bahdanau): {score.shape}.')\n",
        "    # score shape: (batch_size, 49, 1)\n",
        "\n",
        "    # One way to understand why we choose the 1st axis for tf.nn.softmax\n",
        "    # is to note that the 0th axis refers to the batch size, which we want to avoid\n",
        "    # and the 2nd axis is 1-dimensional, so the only option left is the 1st axis.\n",
        "\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "  \n",
        "    if print_stuff:\n",
        "      print(f'The attention weights are obtained from the score tensor (Bahdanau).')\n",
        "      print(f'Shape of attention_weights (Bahdanau): {attention_weights.shape}.')\n",
        "    # attention_weights shape: (batch_size, 49, 1)\n",
        "\n",
        "    context_vector = attention_weights * features\n",
        "\n",
        "    if print_stuff:\n",
        "      print(f'To obtain the shape of the context_vector, we multiply 2 tensors (Bahdanau).')\n",
        "      print(f'Their shapes are (Bahdanau): {attention_weights.shape} and {features.shape}.')\n",
        "      print(f'Shape of context vector before sum (Bahdanau): {context_vector.shape}.')\n",
        "    # context_vector before sum: (batch_size, 49, embedding_dim)\n",
        "\n",
        "    # One way to understand why we choose the 1st axis for tf.reduce_sum\n",
        "    # is to note that the 0th axis refers to the batch size, which we want to avoid\n",
        "    # and the 2nd axis refers to the size of the embedding space that houses our words.\n",
        "    # Recall that the dimension of our vocabulary space is quite large, and we wish\n",
        "    # to simplify matters, which is why we shrink ourselves into the embedding space.\n",
        "    # In the call function for our decoder model, we shall concatenate\n",
        "    # the context vector and the embedded decoder input, so they should agree\n",
        "    # on the size of the embedding dimension!\n",
        "\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    if print_stuff:\n",
        "      print(f'Shape of context vector after sum (Bahdanau): {context_vector.shape}.')\n",
        "    # context_vector after sum: (batch_size, embedding_dim)\n",
        "\n",
        "    # Recall that the context vector is a weighted sum of the cached features, so\n",
        "    # the rightmost dimension must be equal to embedding_dim because the cached features\n",
        "    # passed through a dense layer with (embedding_dim) neurons!\n",
        "\n",
        "    # We emphasize that at this point in the code the context vector is timeless.\n",
        "    # It will inherit time when the call function of the decoder acts on it\n",
        "    # with tf.expand_dims!\n",
        "\n",
        "    if print_stuff:\n",
        "      print(' ')\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C91GG6QbeYiK"
      },
      "source": [
        "# Constructing the decoder model.\n",
        "class RNN_Decoder(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, embedding_dim, units, vocab_size):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.units = units\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    \n",
        "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "\n",
        "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "  def call(self, x, features, hidden, print_stuff):\n",
        "\n",
        "    # The inputs to the decoder call function are:\n",
        "    # a single target word of the intended caption (x), cached features of ResNet-50(features), and\n",
        "    # current hidden state of the GRU (hidden)\n",
        "\n",
        "    # features and hidden are passed through the attention model, which generates\n",
        "    # a context vector, which will form part of the GRU input.\n",
        "    # Note that the context vector currently does not have any time dependence!\n",
        "\n",
        "    # The GRU is also fed the target word, specifically, one entry of a sequential representation of\n",
        "    # a tokenized/padded caption that has been embedded from vocab space to embedded space, which is\n",
        "    # also shared by the features! \n",
        "    # We emphasize that the decoder input is acted on with tf.expand_dims during training/evaluation.\n",
        "    # We also emphasize that the decoder input that is acted on with tf.expand_dims consists\n",
        "    # of a word and its time coordinate!\n",
        "\n",
        "    # We combine the context vector with time (function of current hidden state and cached features) and\n",
        "    # a target word of the caption to generate the input for the GRU.\n",
        "\n",
        "    context_vector, attention_weights = self.attention(features, hidden, print_stuff)\n",
        "\n",
        "    if print_stuff:\n",
        "      print(f'Shape of context_vector (RNN): {context_vector.shape}.')\n",
        "      print(f'Shape of attention_weights (RNN): {attention_weights.shape}.')\n",
        "    # context_vector shape: (batch_size, embedding_dim)\n",
        "    # attention_weights shape: (batch_size, 49, 1)\n",
        "\n",
        "    # Recall that an embedding layer changes an n-index object to an (n+1)-index object,\n",
        "    # where the last index ranges over d2, where the embedding layer takes in a 2-tuple, (d1,d2):\n",
        "    # input: (..., n) -> output shape: (..., n, d2)\n",
        "\n",
        "    if print_stuff:\n",
        "      print(f'Shape of decoder input before embedding layer (RNN): {x.shape}.')\n",
        "    # decoder input before embedding: (batch_size, 1)\n",
        "\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    if print_stuff:\n",
        "      print(f'Shape of decoder input after embedding layer (RNN): {x.shape}.')\n",
        "    # decoder input after embedding: (batch_size, 1, embedding_dim)\n",
        "\n",
        "    # Concatenation along some axis is only possible if the dimensions of all the other axes\n",
        "    # are equal. The resulting concatenated dimension is the sum of the dimensions invovled\n",
        "    # along that particular axis.\n",
        "\n",
        "    if print_stuff:\n",
        "      print(f'Shape of expanded context vector (RNN): {tf.expand_dims(context_vector, 1).shape}')\n",
        "    # Shape of context vector with time: (batch_size, 1, embedding_dim)\n",
        "\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    if print_stuff:\n",
        "      print(f'Shape of concatenation of expanded context vector with time and embedded decoder input (RNN): {x.shape}.')\n",
        "      print(f'Shape of input to GRU (RNN): {x.shape}.')\n",
        "    # concatenation shape: (batch_size, 1, embedding_dim) + (batch_size, 1, embedding_dim) = (batch_size, 1, 2*embedding_dim)\n",
        "\n",
        "    # The GRU outputs an output and its hidden state, which we refer to as state.\n",
        "    # Note that before this output can form part of a caption it must pass through 2 layers.\n",
        "    # The last layer ultimately recovers the dimensionality of the vocab space!\n",
        "\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    if print_stuff:\n",
        "      print(f'Shape of output of GRU (RNN): {output.shape}.')\n",
        "      print(f'Shape of state of GRU (RNN): {state.shape}.')\n",
        "    # output shape: (batch_size, 1, units)\n",
        "    # hidden shape: (batch_size, 1, units\n",
        "\n",
        "    x = self.fc1(output)\n",
        "\n",
        "    if print_stuff:\n",
        "      print(f'Shape of output after fc1 (RNN): {x.shape}.')\n",
        "    # output shape after fc1: (batch_size, 1, units)\n",
        "\n",
        "    x = tf.reshape(x, (-1, x.shape[2]))\n",
        "    # Recall that if one of the options of reshape is set equal to -1, then TensorFlow\n",
        "    # is in charge of generating a tensor that respects the total dimensionality, i.e., \n",
        "    # the reshaping of x will take on the form: (n,units), where\n",
        "    # (batch_size * 1 * units) = (n * units)\n",
        "\n",
        "    if print_stuff:\n",
        "      print(f'Shape of output after fc1, reshaping (RNN): {x.shape}.')\n",
        "    # output shape after fc1, reshaping: (batch_size, units)\n",
        "\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    if print_stuff:\n",
        "      print(f'Shape of output after fc1, reshaping, fc2 (RNN): {x.shape}.')\n",
        "    # outshape shape after fc1, reshaping, fc2: (batch_size, vocab_size)\n",
        "\n",
        "    if print_stuff:\n",
        "      print(f'Shape of prediction (RNN): {x.shape}.')\n",
        "      print(f'Shape of hidden state (RNN): {state.shape}.')\n",
        "      print(f'Shape of attention weights (RNN): {attention_weights.shape}.')\n",
        "    # prediction shape: (batch_size, vocab_size)\n",
        "    # hidden state shape: (batch_size, units)\n",
        "    # attention weights shape: (batch_size, 49, 1)\n",
        "    if print_stuff:\n",
        "      print(' ')\n",
        "\n",
        "    return x, state, attention_weights\n",
        "\n",
        "  # This function below unveils to us that the value of \n",
        "  # hidden_size (with no time axis) is equal to units.\n",
        "  \n",
        "  def reset_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS4lstdZeYiK"
      },
      "source": [
        "# Running this function will generate many print statements revealing the shape\n",
        "# of all the moving parts involved of our model.\n",
        "\n",
        "def probing_our_model (print_stuff):\n",
        "  if print_stuff:\n",
        "    embedding_dim = 250\n",
        "    units = 100\n",
        "    top_k = 5000\n",
        "    vocab_size = top_k + 1\n",
        "\n",
        "    encoder = CNN_Encoder(embedding_dim)\n",
        "    decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
        "\n",
        "    for (batch, (img_tensor, target)) in enumerate(dataset_testing):\n",
        "\n",
        "        print(f'This is the nth batch, where n is {batch}.')\n",
        "        print('Note that we have set the batch size to 1 to improve readability.')\n",
        "\n",
        "        hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "\n",
        "        print(f'Shape of the initial hidden state is {hidden.shape}.')\n",
        "        print(f'The initial hidden state is initialized to zero. It is a row vector of {units} zeroes.')\n",
        "        print('Below is the numerical representation of hidden.')\n",
        "        print(hidden.numpy())\n",
        "        print(' ')\n",
        "        \n",
        "        dec_input = tf.expand_dims([tokenizer_val[1].word_index['<start>']] * target.shape[0], 1)\n",
        "        \n",
        "        print((f'Shape of the initial decoder input, which consists of start tokens is {dec_input.shape}.'))\n",
        "        print(f'This is the numerical representation of the initial decoder input: {dec_input}.')\n",
        "        print(f'This is the lexical representation of the initial decoder input: {[tokenizer_val[1].index_word[n] for n in dec_input[0].numpy()]}')\n",
        "        print(' ')\n",
        "\n",
        "        features = encoder(img_tensor, True)\n",
        "\n",
        "        # The 3 will be replaced by target.shape[1], so that we go over all the target words\n",
        "        # that form some caption.\n",
        "\n",
        "        print(' ')\n",
        "\n",
        "        for i in range(1, 3):\n",
        "          print(f'This is the nth batch, where n is {batch}')\n",
        "          print(f'Engaging in a for loop. This is the ith step, where i is {i}.')\n",
        "          print(' ')\n",
        "\n",
        "          predictions, hidden, _ = decoder(dec_input, features, hidden, True)\n",
        "          \n",
        "          the_void = []\n",
        "          for j in range(11):\n",
        "            the_void.append([tokenizer_val[1].index_word[n] for n in target[:,j].numpy()][0])\n",
        "          print('This is part of the caption we are feeding bit by bit to our decoder is right below:')\n",
        "          print(the_void)\n",
        "\n",
        "          dec_input = tf.expand_dims(target[:, i], 1)\n",
        "          \n",
        "          print(f'We are on the ith step, where i is {i}.')\n",
        "          print(f'Shape of the decoder input is {dec_input.shape}.')\n",
        "          print(f'This is the current word we are on: {[tokenizer_val[1].index_word[n] for n in dec_input[0].numpy()]}')\n",
        "          print(f'This is the nth batch, where n is {batch}. We are at the end of the ith step, where i is {i}.')\n",
        "          print(' ')\n",
        "        \n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrX7lF2-v6Wz"
      },
      "source": [
        "probing_our_model(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PYFRjAAeYiL"
      },
      "source": [
        "# Creating an instance of everything we will use for our models and for the training/evaluation phases.\n",
        "\n",
        "# Below are the parameters of our models.\n",
        "\n",
        "embedding_dim = 256 # used by encoder, decoder\n",
        "units = 512 # used by decoder, attention\n",
        "top_k = 5000 # used by tokenizer, decoder\n",
        "vocab_size = top_k + 1 # used by decoder\n",
        "BATCH_SIZE = 64 \n",
        "\n",
        "# Below are the encoder and decoder models\n",
        "\n",
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
        "\n",
        "# Below we have the altered ResNet-50 model we used to generate our cached features.\n",
        "\n",
        "image_model = tf.keras.applications.resnet50.ResNet50(include_top=False, weights='imagenet')\n",
        "new_input = image_model.input \n",
        "hidden_layer = image_model.layers[-1].output\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
        "\n",
        "# As we mentioned at the very beginning, we will not be using our training dataset\n",
        "# because it is too large., so we shall instead use our validation dataset.\n",
        "# We also will be needing the associated tokenizer.\n",
        "# We EMPHASIZE that the variables below do not originate from the training dataset even\n",
        "# though we refer to them as (something)_train. They originate from the validation dataset!\n",
        "\n",
        "captions_and_images_train = captions_and_images('val')\n",
        "tokenizer_creation = tokenize_pad(captions_and_images_train[0])\n",
        "tokenizer_train = tokenizer_creation[1]\n",
        "sequences_train = tokenizer_creation[2]\n",
        "dataset_creation = create_dataset('val') # the default batch size for create dataset is 64\n",
        "dataset_train = dataset_creation[0]\n",
        "pro_img_and_tok_cap_train = dataset_creation[1]\n",
        "# the above outputs a 4-tuple: (img_name_train, cap_train, img_name_val, cap_val)\n",
        "# we used the first 2 entries to create our dataset\n",
        "# we use the last 2 entries to validate our dataset\n",
        "val_image_name = pro_img_and_tok_cap_train[2]\n",
        "val_caption = pro_img_and_tok_cap_train[3]\n",
        "\n",
        "num_steps = len(pro_img_and_tok_cap_train[0]) // BATCH_SIZE # we use floor division\n",
        "max_length = calc_max_length(sequences_train)\n",
        "attention_features_shape = 49"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23TeoaGTeYiL"
      },
      "source": [
        "# We choose a fancy relative of stochastic gradient descent as our optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam() \n",
        "\n",
        "# Our task is ultimately a classification problem, where we have various labels or equivalently, words.\n",
        "\n",
        "# We wish to associate a high probability for the most sensible word, and a low probability for nonsensible words\n",
        "# because the purpose of these words is to form a caption that will describe the input image.\n",
        "\n",
        "# Note that we have not one-hot encoded our words, so we do not use CategoricalCrossentropy.\n",
        "# Instead, we use SparseCategoricalCrossentropy because the numerical representation of the\n",
        "# words that form our captions take value in the integers.\n",
        "\n",
        "# There are two options inside SparseCategoricalCrossentropy.\n",
        "# See https://stackoverflow.com/a/59872518.\n",
        "\n",
        "# 1. from_logits: We set this equal to True in order to let TensorFlow know that our outputs are\n",
        "# not normalized. If we were to normalize our outputs, then our tokenizer would suffer greatly because\n",
        "# it does not like numbers living in the unit interval\n",
        "\n",
        "# 2. reduction: Setting reduction to 'none' allows us to gain access to the categorical cross entropy\n",
        "# results. We do this in order to take into account padding. We discuss this in detail below.\n",
        "# See https://stackoverflow.com/questions/47057361/how-do-i-mask-a-loss-function-in-keras-with-the-tensorflow-backend.\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "\n",
        "  # First, we ascertain if the numerical representation of the ground truth caption coincides with 0.\n",
        "  # This is False when we focus on the actual caption or its special tokens, <start> and <end>.\n",
        "  # This is True when we focus on the special token, <pad> because its associated index is 0.\n",
        "  # Recall that the <pad> token was introduced to have all captions be of the same length.\n",
        "  # By negating this sequence of Booleans, we essentially create a filter, whose purpose\n",
        "  # is to set the loss coming from the padding sector equal to zero.\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  \n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  # In order to multiply our cross entropy by our mask, we must make sure that our mask\n",
        "  # has the same data type as loss_.\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype) \n",
        "  \n",
        "  loss_ *= mask # We apply our padding filter.\n",
        "\n",
        "  return tf.reduce_mean(loss_) # We compute the mean of our cross entropy results."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZbeZElceYiM"
      },
      "source": [
        "# We create checkpoints, so we can save/restore our progress to/from disk.\n",
        "\n",
        "# First, we create a file path to our checkpoint.\n",
        "checkpoint_path = os.path.abspath('.') + \"/checkpoints/training\"\n",
        "\n",
        "# Next, we make sure that our checkpoint has access to our encoder, decoder, and optimizer.\n",
        "ckpt = tf.train.Checkpoint(encoder=encoder, decoder=decoder, optimizer=optimizer)\n",
        "\n",
        "# Lastly, we use a checkpoint manager, which combines everything above and places\n",
        "# a limit to the number of checkpoints we can save. These are the 3 inputs to\n",
        "# our checkpoint manager.\n",
        "\n",
        "# checkpoint: our tf.train.Checkpoint, which houses our encoder, decoder, and optimizer.\n",
        "# directory: the directory where our checkpoints will be saved to.\n",
        "# max_to_keep: the maximum number of checkpoints to keep.\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8S94lGdeYiM"
      },
      "source": [
        "# We initialize our starting epoch to be 0.\n",
        "start_epoch = 0\n",
        "\n",
        "# However, if there are checkpoints saved to disk, then we can recover them using the if statement below.\n",
        "\n",
        "# ckpt_manager.latest_checkpoint returns the path to the latest checkpoint if it exists (if statement executed).\n",
        "# ckpt_manager.latest_checkpoint returns None if there are no checkpoints (if statement not executed).\n",
        "\n",
        "# The function below will dictate if we restore a checkpoint or not.\n",
        "def restore_checkpoint (answer):\n",
        "  if answer:\n",
        "    if ckpt_manager.latest_checkpoint:\n",
        "      # We use the string method, .split to partition the filepath generated by .latest_checkpoint.\n",
        "      # By setting the separator equal to a hyphen, we generate a list of the form:\n",
        "      # ['./checkpoints/train/ckpt', 'n'], where n is the epoch of interest.\n",
        "      # We can access this epoch by tacking on a [-1].\n",
        "      start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
        "      # Below we restore the latest checkpoint, which is saved to disk here: ckpt_manager.latest_checkpoint.\n",
        "      ckpt.restore(ckpt_manager.latest_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgXDZjyx4Uuy"
      },
      "source": [
        "# Running the restore_checkpoint function above might recover a latest checkpoint.\n",
        "# We avoid doing this because we wish to train our model from scratch, but most importantly,\n",
        "# because we have made important changes to our data.\n",
        "\n",
        "# Below we check what epoch we are currently at.\n",
        "if start_epoch > 0:\n",
        "  print(f'Path to latest checkpoint: {ckpt_manager.latest_checkpoint}.')\n",
        "  print(f'We are living in the nth epoch with n={start_epoch}.')\n",
        "  print('We have been transported to the past!')\n",
        "else:\n",
        "  print(f'We are living in the nth epoch with n={start_epoch}.')\n",
        "  print('We must start at the beginning of time!')\n",
        "print(' ')\n",
        "print(f'This is where we save our checkpoints: {checkpoint_path}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQVgW0wUeYiN"
      },
      "source": [
        "# We will generate a plot showcasing our loss as a function of epochs, so\n",
        "# we collect our losses in the list below.\n",
        "loss_plot = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddwvVK-ieYiO"
      },
      "source": [
        "# The training phase\n",
        "\n",
        "# @tf.function promotes our train_step function, which gives us access\n",
        "# to other TensorFlow features. We do not investigate these features in this notebook.\n",
        "\n",
        "@tf.function\n",
        "\n",
        "# train_step has 2 inputs:\n",
        "# img_tensor: a tensor that houses the extracted features generated by some image passing through an altered ResNet-50 model\n",
        "# target: a tensorial of shape that houses a numerical representation of a caption\n",
        "\n",
        "def train_step(img_tensor, target):\n",
        "\n",
        "  loss = 0\n",
        "\n",
        "  # We initialize the hidden state of our decoder to 0 because any 2 image captions are independent of each other.\n",
        "  hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "\n",
        "  # We initialize the decoder input by generating an array containing (batch_size) start tokens.\n",
        "  # Then, we expand the shape of the input with tf.expand_dims, so that it has a sense of time.\n",
        "  dec_input = tf.expand_dims([tokenizer_train.word_index['<start>']] * target.shape[0], 1)\n",
        "\n",
        "  # We use the line below in order to keep track of certain variables, which we refer to as trainable_variables.\n",
        "  with tf.GradientTape() as tape: \n",
        "\n",
        "      features = encoder(img_tensor, False) \n",
        "\n",
        "      # The for loop below ranges from 1 to the maximum length of a tokenized/padded caption in our\n",
        "      # dataset, which equals 61. The index, i will be used to access the entries of our target caption.\n",
        "      for i in range(1, target.shape[1]):\n",
        "\n",
        "          # Note that we are actually tackling multiple examples simulatenously because batch_size = 64.\n",
        "          # Our decoder takes in the following inputs.\n",
        "          # dec_input: a numerical representation of a tokenized/padded caption\n",
        "          # features: extracted features from ResNet-50\n",
        "          # hidden: the decoder's hidden state\n",
        "          predictions, hidden, _ = decoder(dec_input, features, hidden, False)\n",
        "\n",
        "          # Below we calculate the cross entropy (sparse categorical) with respect to\n",
        "          # predictions and the intended word of our target caption.\n",
        "          loss += loss_function(target[:, i], predictions)\n",
        "\n",
        "          # By using teacher forcing, we choose the next decoder input to equal the\n",
        "          # target word of this iteration, i.e., we show the model the answer for this round.\n",
        "          dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "  # We do not understand total_loss.\n",
        "  # Generating an integer from target.shape[1] in order to execute division.\n",
        "  total_loss = (loss / int(target.shape[1]))\n",
        "  \n",
        "  # Here, we can appreciate easily why we would want to inherit attributes and methods from tf.keras.Model.\n",
        "  # tf.keras.Model inherits from tf.Module.\n",
        "  # tf.Module has an attribute called trainable_variables, which gives us access to all the variables\n",
        "  # that are being trained. Hence, we can now differentiate with respect to the variables of both\n",
        "  # our encoder and decoder models.\n",
        "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  # We access the tape from above in order to compute the gradient of our loss function\n",
        "  # with respect to our trainable variables.\n",
        "  gradients = tape.gradient(loss, trainable_variables) \n",
        "\n",
        "  # Lastly, we apply the gradients we just calculated in order to take a step along the gradient and towards the descent.\n",
        "  optimizer.apply_gradients(zip(gradients, trainable_variables)) \n",
        "\n",
        "  return loss, total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-00aJqUleYiO"
      },
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "# We train our model through the entire dataset 20 times.\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "\n",
        "    # We start a timer, so we can share how long it takes for our model to train\n",
        "    # through the entire dataset, i.e., the duration of a epoch.\n",
        "    start = time.time()\n",
        "\n",
        "    # We initiate the total loss to 0.\n",
        "    total_loss = 0\n",
        "\n",
        "    # This for loop allows us to iterate through the entire dataset.\n",
        "    for (batch, (img_tensor, target)) in enumerate(dataset_train):\n",
        "\n",
        "        # We record both the batch loss and the total loss, so we can share the average batch\n",
        "        # loss and so we can generate a plot of the loss versus the number of epochs.\n",
        "        batch_loss, t_loss = train_step(img_tensor, target)\n",
        "        total_loss += t_loss\n",
        "\n",
        "        # If the current batch is divisible by 100, we generate the following summary.\n",
        "        if batch % 100 == 0: \n",
        "            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
        "            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
        "    \n",
        "    # We store our total loss after training through the entire dataset.\n",
        "    # As we mentioned before, this is done in order to create a plot of the loss\n",
        "    # versus the number of epochs.\n",
        "    loss_plot.append(total_loss / num_steps)\n",
        "\n",
        "    # If the current epoch is even, we call our checkpoint manager and\n",
        "    # the configurations of our encoder, decoder, and optimizer.\n",
        "    if epoch % 2 ==0:\n",
        "        ckpt_manager.save()\n",
        "\n",
        "    # Due to the nature of Python's range function, we will not be able to save the data\n",
        "    # from the 20th epoch. So, we insert another if statement in order to guarantee\n",
        "    # that our checkpoint manager sees that data.\n",
        "    if epoch == EPOCHS - 1:\n",
        "      ckpt_manager.save()\n",
        "\n",
        "    # We finish off this iteration of the for loop by providing the following summary.\n",
        "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n",
        "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6PCQ5ZppN1P"
      },
      "source": [
        "# Generating our plot with the appropiate labels and title\n",
        "plt.plot(loss_plot)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Plot')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxVNGF21pP7q"
      },
      "source": [
        "# The evaluation phase\n",
        "\n",
        "def evaluate(image):\n",
        "\n",
        "    # We have already calculated the max_length and attention_features_shape.\n",
        "    # I do not know why the dimensions of the attention plot are determined\n",
        "    # by max_length and attentions_features_shape.\n",
        "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
        "\n",
        "    # We are evaluating a single image, so our batch consists of a single image!\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "\n",
        "    # load_image(image)[0] generates a tensor of shape, (224, 224, 3) with entries haveing dtype = float32.\n",
        "    # The aftermath of tf.expand_dims is a tensor of shape: (1, 224, 224, 3).\n",
        "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
        "\n",
        "    # We defined image_features_extract_model earlier as ResNet-50 without its top layer.\n",
        "    # We run over image through our altered ResNet-50 model.\n",
        "    img_tensor_val = image_features_extract_model(temp_input)\n",
        "\n",
        "    # We reshape our tensor to recover a tensor of shape: (1, 49, 2048).\n",
        "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],\n",
        "                                                 -1,\n",
        "                                                 img_tensor_val.shape[3]))\n",
        "\n",
        "    # We pass the image tensor above through our encoder.\n",
        "    features = encoder(img_tensor_val, False)\n",
        "\n",
        "    # We initialize the decoder input to be the start token.\n",
        "    dec_input = tf.expand_dims([tokenizer_train.word_index['<start>']], 0)\n",
        "\n",
        "    # We save the generated caption to a list.\n",
        "    result = []\n",
        "\n",
        "    for i in range(max_length):\n",
        "\n",
        "        # Saving the outputs of our decoder model.\n",
        "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden, False)\n",
        "\n",
        "        # Having generated attention weights, we generate our attention plot.\n",
        "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
        "\n",
        "        # Note that our decoder outputs a categorical probability distribution, which\n",
        "        # we refer to as predictions. This is a discrete distribution with 5001 categories.\n",
        "        # We emphasize that the number of categories is determined by the size of our vocabulary.\n",
        "\n",
        "        # We take a random sample from our categorical probability distribution, predictions.\n",
        "        # Then, we tack on a .numpy() at the end in order to generate a number.\n",
        "        # This number is an index that can be fed into our tokenizer.\n",
        "        # Lastly, we record the word that the tokenizer generates in our result list.\n",
        "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
        "        result.append(tokenizer_train.index_word[predicted_id])\n",
        "\n",
        "        # If the index that we generate from sampling our distribution\n",
        "        # recovers the end token, we return the caption and the plot.\n",
        "        if tokenizer_train.index_word[predicted_id] == '<end>':\n",
        "            return result, attention_plot\n",
        "\n",
        "        # We update the decoder input to be the word we just generated.\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    # The x-axis of the attention plot is governed by max_length.\n",
        "    # It is possible that the caption we generate has a length that is smaller\n",
        "    # than the max_length, so we trim the x-axis of our attention plot.\n",
        "    # However, we keep full size of the attention features shape.\n",
        "    attention_plot = attention_plot[:len(result), :]\n",
        "    return result, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Achg68CMPWBL"
      },
      "source": [
        "# We will describe this cell later.\n",
        "def plot_attention(image, result, attention_plot):\n",
        "    temp_image = np.array(Image.open(image))\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "\n",
        "    len_result = len(result)\n",
        "    for i in range(len_result):\n",
        "        temp_att = np.resize(attention_plot[i], (8, 8))\n",
        "        grid_size = max(np.ceil(len_result/2), 2)\n",
        "        ax = fig.add_subplot(grid_size, grid_size, i+1)\n",
        "        ax.set_title(result[i])\n",
        "        img = ax.imshow(temp_image)\n",
        "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGki2CyYrXiW"
      },
      "source": [
        "# We will describe this cell later.\n",
        "rid = np.random.randint(0, len(val_image_name))\n",
        "image = val_image_name[rid]\n",
        "real_caption = ' '.join([tokenizer_train.index_word[i]\n",
        "                        for i in val_caption[rid] if i not in [0]])\n",
        "result, attention_plot = evaluate(image)\n",
        "\n",
        "print('Real Caption:', real_caption)\n",
        "print('Prediction Caption:', ' '.join(result))\n",
        "plot_attention(image, result, attention_plot)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}