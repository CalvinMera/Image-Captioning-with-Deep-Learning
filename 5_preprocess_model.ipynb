{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Listing the paths, subpaths, and filenames of interest.\n",
    "annotations_intermediate_path = '/data/annotations/'\n",
    "# The dictionary, annotations_ending_path includes the training and validation .json files that have been processed,\n",
    "# namely, precanned and rejected captions have been taken of and any image with 2 or less captions have been removed.\n",
    "annotations_ending_path = {'test':'test.json', 'train':'train_cleaned.json', 'val':'val_cleaned.json'}\n",
    "images_intermediate_path = {'test':'/data/images/test/', 'train':'/data/images/train/', 'val':'/data/images/val/'}\n",
    "\n",
    "# Generating 2-tuples, where the first entry houses the captions and the second entry houses the associated image paths.\n",
    "# We shall do this again once we have processed the images according to the requirements of ResNet-50 and\n",
    "# once we have tokenized and padded our captions.\n",
    "\n",
    "# In order to shorten computing time, we only use the validation dataset, which has 6,982 images and 32,299 captions.\n",
    "# We take 80% of those examples for the training phase, and the rest for the validation phase.\n",
    "# Note that we did train on the entire dataset (21,220 images and 98,015 captions) for a single epoch.\n",
    "# This took almost 4.5 hours to complete.\n",
    "\n",
    "def captions_and_images (dataset):\n",
    "\n",
    "# First, we open the data set, which is saved as a .json file.\n",
    "    with open(os.path.abspath('.') + annotations_intermediate_path + annotations_ending_path[dataset], 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "# Translating the .json file from the above to a Pandas data frame.\n",
    "    df = pd.DataFrame.from_dict(data, orient='index')\n",
    "    df = df.reset_index() # Before we can iterate over the index, indices of neighboring entries must differ by 1.\n",
    "    # Neighboring indices do not always differ by 1 since we have removed images from our dataset.\n",
    "    df = df.drop(['index'], axis = 1) # Resetting the index creates a column that houses the previous indices, so we drop it.\n",
    "\n",
    "# We collect the image path and caption of every row.\n",
    "# Then, we form a dictionary, whose keys are the image paths and whose values are the associated captions.\n",
    "    image_path_to_caption = collections.defaultdict(list)\n",
    "    for n in range(df.shape[0]):\n",
    "        image_path = os.path.abspath('.') + images_intermediate_path[dataset] + df.file_name.iloc[n]\n",
    "        caption = f'<start> {df.caption.iloc[n]} <end>' # The curly brackets ensures that outputs are turned into text.\n",
    "        image_path_to_caption[image_path].append(caption)\n",
    "\n",
    "# We now separate the captions and image paths in 2 lists.\n",
    "    captions = [] \n",
    "    img_name_vector = []\n",
    "\n",
    "    for image_path in image_path_to_caption.keys(): # The keys are unique, so we only see each image path once.\n",
    "        caption_list = image_path_to_caption[image_path] # Each image has at least 3 captions.\n",
    "        captions.extend(caption_list) # We add the captions to the captions list.\n",
    "        img_name_vector.extend([image_path]*len(caption_list)) # We add the image path multiple times to reflect that images have\n",
    "        # multiple captions.\n",
    "    return captions, img_name_vector"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# The function below will output a preprocessed image ready for ResNet-50 and its path.\n",
    "def load_image (image_path):\n",
    "    image = tf.io.read_file(image_path) # loads the raw data as a string\n",
    "    image = tf.image.decode_jpeg(image, channels = 3) # decodes JPEG-encoded image to a uint8 (8-bit unsigned integer) tensor\n",
    "    image = tf.image.resize(image, (224,224)) # resizes the tensor above to agree with the input size of ResNet-50\n",
    "    image = tf.keras.applications.resnet50.preprocess_input(image) # Take cares of other ResNet-50 preprocessing steps, such as\n",
    "    # going from a RGB to a BGR channel. Also, every channel is zero-centered relative to the ImageNet data set.\n",
    "    # What does it mean to zero-center a color channel?\n",
    "    return image, image_path"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# The output of load_image is:\n",
    "# tf.Tensor of shape == (224, 224, 3), whose entries are float32 and the image path.\n",
    "# The first index accesses the img_name_vector entry, the second index acceses the 0th file path, and\n",
    "# the third index accesses the processed image.\n",
    "# load_image(captions_and_images('train')[1][0])[0]\n",
    "# load_image(captions_and_images('train')[1][0])[1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Creating a model where the input layer will receive our images and the output layer is the last convolutional layer of ResNet50.\n",
    "# After the images have run its course through the model, we store the output as a vector, which contains all the extracted features.\n",
    "# We save these extracted features to disk.\n",
    "\n",
    "def cache_extracted_features (dataset):\n",
    "\n",
    "# We instantiate the ResNet-50 model without the top layer and load the ImageNet weights.\n",
    "# Setting include_top to False results in dropping these 2 layers: avg_pool (GlobalAveragePooling2) and predictions (Dense).\n",
    "    image_model = tf.keras.applications.resnet50.ResNet50(include_top=False, weights='imagenet')\n",
    "\n",
    "    new_input = image_model.input # We grab the input of the model.\n",
    "    hidden_layer = image_model.layers[-1].output # We grab the last layer's output.\n",
    "\n",
    "# We build a model using the functional API whose input and output are defined above.\n",
    "    image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
    "\n",
    "# We extract only the unique paths and then sort them.\n",
    "    encode_data = sorted(set(img_name_vector))\n",
    "\n",
    "# We use the data API to generate a sequence of data items.\n",
    "# Note that as of now the image_dataset is a sequence of tensors that contain strings reflecting our file paths.\n",
    "    image_dataset = tf.data.Dataset.from_tensor_slices(encode_data)\n",
    "\n",
    "# We can take a peak of the first element of this dataset by running the following code:\n",
    "# for item in image_dataset.take(1):\n",
    "#     print(item)\n",
    "# The output for the training data set is:\n",
    "# tf.Tensor(b'/Users/calvin/projects/mlp_1/data/images/train/VizWiz_train_00000000.jpg', shape=(), dtype=string)\n",
    "\n",
    "# We use the map method to apply the load_image function to our dataset.\n",
    "\n",
    "# tf.data.AUTOTUNE ensures that the number of batches that are computed in parallel is based on available sources.\n",
    "# The batch method sets the number of images that are dealt with in a single batch.\n",
    "    image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n",
    "\n",
    "# After applying the load_image function to our dataset, we obtain images that have been processed,\n",
    "# i.e., they have been read, decoded, resized, and preprocessed according to the requirements of ResNet-50.\n",
    "\n",
    "# As before, we can see the output of this newly transformed image_dataset with the following code:\n",
    "# for item in image_dataset.take(1):\n",
    "#    print(item)\n",
    "# The output is a tf.Tensor of shape == (batch_size, 224, 224, 3) with (batch_size) image paths.\n",
    "\n",
    "# Now that we have a sequence of data items, specifically, a preprocessed image and its associated file path we perform\n",
    "# the following for loop.\n",
    "\n",
    "# First, we extract the features of our image. Then, we resize the extracted features.\n",
    "    for img, path in tqdm(image_dataset):\n",
    "        batch_features = image_features_extract_model(img) \n",
    "        batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3])) \n",
    "\n",
    "# We perform another for loop, but this time it is not over the 2-tuple, (image, path) it is over the 2-tuple,\n",
    "# (extracted features, paths). Lastly, we save the path to a binary file using the file format, (...).npy.\n",
    "        for bf, p in zip(batch_features, path):\n",
    "            path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "            np.save(path_of_feature, bf.numpy())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We use list comprehension to generate the lengths of all the elements of the input, tensor.\n",
    "# Then, we return the element with the longest length.\n",
    "# Note that writing max([...]) is valid too, but one can get away with leaving out the square brackets.\n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We tokenize and pad our captions.\n",
    "def tokenize_pad (captions):\n",
    "    \n",
    "    top_k = 5000 # We set a limit to the number of words we hold in our vocabulary, namely only the (top_k - 1) frequent words.\n",
    "\n",
    "# num_words: determines the maximum number of words to keep, i.e., only the (num_words - 1) most common words are kept.\n",
    "# oov_token: if we encounter a word that is not part of our vocabulary, then we assign it the special token, <unk>.\n",
    "# filters: a string of characters that are filtered from the text being analyzed.\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k, oov_token=\"<unk>\", filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~')\n",
    "\n",
    "# We update our internal vocabulary based on the captions we extracted from the data frame above.\n",
    "# Note that we feed a list of captions to our model, not a sequence, so we use fit_on_texts, rather than fit_on_sequences.\n",
    "    tokenizer.fit_on_texts(captions)\n",
    "\n",
    "# Creating a word to index and index to word mappings.\n",
    "# We reserve the index, 0 for the special token, <pad>.\n",
    "    tokenizer.word_index['<pad>'] = 0\n",
    "    tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "# Transforming our collection of texts into a sequence of integers.\n",
    "# As we mentioned above, only the (num_words - 1) most frequent word are taken into account.\n",
    "    seqs = tokenizer.texts_to_sequences(captions)\n",
    "    max_length = calc_max_length(seqs)\n",
    "\n",
    "# Note that we can check the inner workings of our tokenizer with the following methods and attributes of Tokenizer:\n",
    "# get_config(), index_word, word_index.\n",
    "\n",
    "# Note that we did not lower the characters of our string because this is taken care by the default settings of\n",
    "# tf.keras.preprocessing.text.Tokenizer.\n",
    "\n",
    "# One can access the index to word mapping and recover the 0th caption by first accessing the indices of the 0th entry\n",
    "# of seqs and passing that sequence of integers into the index to word mapping. This can be replicated for padded captions, too.\n",
    "\n",
    "# Not all sequences share the same length, so we first find the longest sequence, and then we add 0's to all sequences shorter than\n",
    "# the longest sequence.\n",
    "    cap_vector = tf.keras.preprocessing.sequence.pad_sequences(seqs, padding='post') \n",
    "\n",
    "    return cap_vector, tokenizer, seqs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We make sure that we can retrieve a functioning tokenizer from above because we will be accessing the tokenizer\n",
    "# during the training phase.\n",
    "# tokenizer_val= tokenize_pad(captions_and_images('val')[0])\n",
    "# We print the first 8 indices and their associated words.\n",
    "# for n in range(8):\n",
    "#     print(n, tokenizer_val[1].index_word[n])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Confirming that our choice of num_words is fulfilled.\n",
    "# largest = 0\n",
    "# for n in range(len(tokenizer_val[0])):\n",
    "#     temp = np.max(tokenizer_val[0][n])\n",
    "#     if temp > largest:\n",
    "#         largest = temp\n",
    "# print(largest)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# captions_test = captions_and_images('val')[0][0]\n",
    "# print(captions_test) # generating the 0th caption\n",
    "# print([tokenizer_val[1].index_word[n] for n in tokenizer_val[2][0]]) # generating the 0th caption (tokenized)\n",
    "# tokenized_padded_test = [tokenizer_val[1].index_word[n] for n in tokenizer_val[0][0]]\n",
    "# print(tokenized_padded_test) # generating the 0th caption (tokenized and padded)\n",
    "# print(len(tokenized_padded_test)) # generating the maximum length of the tokenized/padded captions from the validation set"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# The maximum lengths of our tokenized/padded captions for the training set is 115.\n",
    "# The maximum lengths of our tokenized/padded captions for the validation set is 157.\n",
    "# The second zero accesses cap_vector, which is array where each row is a caption,\n",
    "# so the third zero accesses the 0th tokenized/padded caption.\n",
    "# print(len(tokenize_pad(captions_and_images('train')[0])[0][0]))\n",
    "# print(len(tokenize_pad(captions_and_images('val')[0])[0][0]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Just as did before with captions_and_images, we generate 2-tuples, where\n",
    "# the first entry are the image paths to the preprocessed images and\n",
    "# the second entry are the associated tokenized and padded captions.\n",
    "\n",
    "def pro_img_and_tok_cap (dataset):\n",
    "\n",
    "# Just as before with captions_and_images, the pattern we follow is:\n",
    "# Generate a dictionary, where the keys are the images and the values are all of its associated captions (at least 3).\n",
    "# Generate 2 lists, where the first holds the captions and the second holds the images (multiplicity taken into account).\n",
    "    img_to_cap_vector = collections.defaultdict(list) # keys are image paths, values are tokenized/padded captions.\n",
    "    \n",
    "# Recall that the output of captions_and_image(dataset) is (captions, img_name_vector).\n",
    "# Recall that the output of tokenize_pad(captions) is cap_vector, tokenizer, seqs.\n",
    "\n",
    "# img_name_vector is captions_and_images(dataset)[1].\n",
    "# First, define captions_and_images_data = captions_and_images('data') ('train' or 'val').\n",
    "# Second, captions_and_images_data[1] gives the img_name_vector for the dataset, data.\n",
    "\n",
    "# cap_vector is tokenize_pad(captions)[0].\n",
    "# First, define tokenize_pad_data = tokenize_pad(captions_and_images_data[0]).\n",
    "# Note that tokenize_pad_data[0] is cap_vector for the dataset, data.\n",
    "\n",
    "    captions_and_images_data = captions_and_images(dataset)\n",
    "    img_name_vector = captions_and_images_data[1]\n",
    "    cap_vector = tokenize_pad(captions_and_images_data[0])[0]\n",
    "\n",
    "    for img, cap in zip(img_name_vector, cap_vector):\n",
    "        img_to_cap_vector[img].append(cap)\n",
    "\n",
    "    # We now create the training and validation split.\n",
    "    img_keys = list(img_to_cap_vector.keys()) # first we generate the unique keys, i.e., the unique image paths.\n",
    "    random.shuffle(img_keys) # shuffling all the keys\n",
    "    slice_index = int(len(img_keys)*0.8) # we create an indexing scheme that separates the first 80 percent of the recently shuffled dataset\n",
    "    \n",
    "    img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n",
    "    \n",
    "    # We now create 2 lists per dataset (training and validation), which house the images and captions.\n",
    "\n",
    "    img_name_train = []\n",
    "    cap_train = []\n",
    "\n",
    "    for imgt in img_name_train_keys:\n",
    "        capt_len = len(img_to_cap_vector[imgt])\n",
    "        img_name_train.extend([imgt] * capt_len)\n",
    "        cap_train.extend(img_to_cap_vector[imgt])\n",
    "\n",
    "    img_name_val = []\n",
    "    cap_val = []\n",
    "\n",
    "    for imgv in img_name_val_keys:\n",
    "        capv_len = len(img_to_cap_vector[imgv])\n",
    "        img_name_val.extend([imgv] * capv_len)\n",
    "        cap_val.extend(img_to_cap_vector[imgv])\n",
    "    \n",
    "    return img_name_train, cap_train, img_name_val, cap_val"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We list parameters which will be used later on.\n",
    "\n",
    "# BATCH_SIZE = 64\n",
    "# BUFFER_SIZE = 1000 or buffer_size=tf.data.AUTOTUNE\n",
    "\n",
    "# embedding_dim = 256 (used in the CNN encoder and RNN decoder models)\n",
    "# units = 512 (used in the RNN decoder and Bahdanau attention models)\n",
    "# vocab_size = top_k + 1 (used in the RNN decoder model)\n",
    "\n",
    "# num_steps equals the number of captions in our dataset divided (integer or floor) by the batch size.\n",
    "# The number of captions is 98,015 and 32,299 for the training and validation dataset, respectively.\n",
    "# These values can be found when we preprocess the data in 3_preprocessing_train_val.ipynb.\n",
    "\n",
    "# num_steps = len(pro_img_and_tok_cap(dataset)[0]) // BATCH_SIZE\n",
    "\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048).\n",
    "# Shape of the vector extracted from ResNet50 is (49, 2048).\n",
    "\n",
    "# features_shape = 2048\n",
    "# attention_features_shape = 49"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# The function below is used to load the numpy files, which house the extracted features.\n",
    "# Note that the output is the extracted features of the processed image and its associated caption.\n",
    "def map_func(img_name, cap):\n",
    "  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "  return img_tensor, cap"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We now create a tf.data.Dataset dataset which will be fed into our models.\n",
    "# We added an option to control the batch_size in order to easily generate the dimensionality of the variables.\n",
    "def create_dataset (data):\n",
    "    # Creating a sequence of data items.\n",
    "    _pro_img_and_tok_cap = pro_img_and_tok_cap(data)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((_pro_img_and_tok_cap[0], _pro_img_and_tok_cap[1])) # single positional argument!\n",
    "\n",
    "    # Use map to load the numpy files holding the cached features in parallel.\n",
    "    # We use tf.numpy_function to promote the Python function to a TensorFlow operation.\n",
    "    dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "            map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # Shuffling and creating batches\n",
    "    dataset = dataset.shuffle(1000).batch(64) \n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset, _pro_img_and_tok_cap"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Run this block in order to take a look at the shape of the outputs of create_dataset.\n",
    "# We do this for the validation dataset and set the batch size to 2 in order to shorten the computation time.\n",
    "# dataset_val_test = create_dataset('val', 2)\n",
    "# for item in dataset_val_test.take(1):\n",
    "#     print(item)\n",
    "# The outputs are as follows:\n",
    "# The first entry is the processed image: tf.Tensor of shape == (batch_size = 2, 49, 2048), float32\n",
    "# The second entry is the tokenized/padded caption: tf.Tensor of shape == (batch_size = 2 , 157), int32"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We also investigate the shape of several variables that will play a crucial role in what follows.\n",
    "# count = 0\n",
    "# for (batch, (img_tensor, target)) in enumerate(dataset_val_test):\n",
    "#     if count == 1:\n",
    "#         break\n",
    "#     print(batch)\n",
    "#     print(f'This is the shape of img_tensor, {img_tensor.shape}')\n",
    "#     print(f'This is the shape of target, {target.shape}')\n",
    "#     x = tf.expand_dims([tokenizer_val[1].word_index['<start>']] * target.shape[0], 1) # initial decoder input\n",
    "#     print(f'This is the shape of x, {x.shape}')\n",
    "#     # print(x)\n",
    "#     y = target[1,:]\n",
    "#     print(f'This is the shape of y, {y.shape}')\n",
    "#     # print(y)\n",
    "#     # print([tokenizer_val[1].index_word[n] for n in y.numpy()])\n",
    "#     z = tf.expand_dims(target[:,2],1) # decoder input a la teaching forcing\n",
    "#     print(f'This is the shape of z, {z.shape}')\n",
    "#     # print(z)\n",
    "#     count += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We are now ready to tackle our encoder-decoder model.\n",
    "# First, we built the CNN encoder.\n",
    "\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "\n",
    "    # We are subclassing tf.keras.Model so we define our layers in __init__ and any operations this model\n",
    "    # will carry out in the call function, which can be executed by entering: CNN_Encoder(embedding_dim)(x).\n",
    "\n",
    "    # The syntax has changed in Python 3.0, so we can replace super(CNN_Encoder, self).__init__() with super().__init__()\n",
    "    \n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        # x before fc: (batch_size, 49, 2048)\n",
    "        # x after fc == (batch_size, 49, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x, print_stuff):\n",
    "        if print_stuff:\n",
    "            print(f'Shape of img_tensor before dense layer and ReLU (CNN): {x.shape}.')\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        if print_stuff:\n",
    "            print(f'Shape of img_tensor after dense layer and ReLU (CNN): {x.shape}.')\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Execute the block below to see the output shape of the encoder.\n",
    "# for (batch, (img_tensor, target)) in enumerate(dataset_val_test):\n",
    "#     x = CNN_Encoder(256)(img_tensor, True)\n",
    "#     break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Building the attention model.\n",
    "\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, units):\n",
    "    super().__init__()\n",
    "    # We are implementing Bahdanau attention so we will need 2 layers, so that the hidden state and the encoder output each\n",
    "    # get assigned and multiplied by their own weights.\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    \n",
    "    # The last layer has a single neuron because we wish to generate weights that are numbers, not tensors.\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "    \n",
    "  def call(self, features, hidden, print_stuff):\n",
    "    # features: (batch_size, 49, embedding_dim)\n",
    "    # hidden: (batch_size, hidden_size = units) these units are inherits from GRU\n",
    "    # hidden_with_time_axis: (batch_size, 1, hidden_size = units) time is a scalar, so a single number will suffice\n",
    "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "    if print_stuff:\n",
    "      print(f'Shape of hidden_with_time_axis (Bahdanau): {hidden_with_time_axis.shape}.')\n",
    "\n",
    "    W1_product = self.W1(features)\n",
    "    if print_stuff:\n",
    "      print(f'Shape of W1_product: {W1_product.shape}.')\n",
    "\n",
    "    W2_product = self.W2(hidden_with_time_axis)\n",
    "    if print_stuff:\n",
    "      print(f'Shape of W2_product: {W2_product.shape}.')\n",
    "\n",
    "    # attention_hidden_layer: (batch_size, 49, hidden_size = units)\n",
    "    # tf.nn.tanh has been replaced by tf.math.tanh\n",
    "    attention_hidden_layer = (tf.math.tanh(W1_product + W2_product))\n",
    "    if print_stuff:\n",
    "      print(f'Shape of attention_hidden_layer (Bahdanau): {attention_hidden_layer.shape}.')\n",
    "\n",
    "    # score: (batch_size, 49, 1)\n",
    "    score = self.V(attention_hidden_layer)\n",
    "    if print_stuff:\n",
    "      print(f'Shape of score (Bahdanau): {score.shape}.')\n",
    "\n",
    "    # attention_weights: (batch_size, 49, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "    if print_stuff:\n",
    "      print(f'Shape of attention_weights (Bahdanau): {attention_weights.shape}.')\n",
    "\n",
    "    # context_vector before sum: (batch_size, 49, embedding_dim)\n",
    "    # context_vector after sum: (batch_size, hidden_size = units)\n",
    "    if print_stuff:\n",
    "      print(f'Shape of features: {features.shape}.')\n",
    "    context_vector = attention_weights * features # element-wise multiplication with broadcasting\n",
    "    if print_stuff:\n",
    "      print(f'Shape of context vector before sum (Bahdanau): {context_vector.shape}.')\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "    if print_stuff:\n",
    "      print(f'Shape of context vector after sum (Bahdanau): {context_vector.shape}.')\n",
    "\n",
    "    return context_vector, attention_weights"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We now built the decoder model.\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, embedding_dim, units, vocab_size):\n",
    "    super().__init__()\n",
    "    self.units = units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "    self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "  def call(self, x, features, hidden, print_stuff):\n",
    "\n",
    "    # The Bahdanau attention model takes in 2 inputs, features and hidden:\n",
    "\n",
    "    # features: the output of the encoder (ResNet-50) acting on our images\n",
    "    # features shape == (batch_size, 49, embedding_dim)\n",
    "\n",
    "    # hidden: the hidden state of our GRU\n",
    "    # hidden shape == (batch_size, hidden_size = units), where\n",
    "    # hidden_size is determined by the number of neurons our GRU has.\n",
    "\n",
    "    # Bahdanu outputs a weighted sum of the encoder outputs (context_vector) and the weights (attention_weights) used in that sum\n",
    "    context_vector, attention_weights = self.attention(features, hidden, print_stuff)\n",
    "\n",
    "    # x is the decoder input, which takes on the form: dec_inc = tf.expand_dims(target[:,i],1)\n",
    "    # x (dec_input): (batch_size, 1)\n",
    "\n",
    "    # Using the tokenizer, one maps the integers featured in the target to a caption, specifically,\n",
    "    # we generate the ith tokenized word of (batch_size) captions partcipating in some batch.\n",
    "\n",
    "    # embedding: changes an n-index object to an (n+1)-index object, where the last index ranges over the output dim\n",
    "    # x before embedding: (batch_size, 1)\n",
    "    # x after embedding: (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "    if print_stuff:\n",
    "      print(f'Shape of decoder input after embedding layer (RNN): {x.shape}.')\n",
    "\n",
    "    # A GRU is fed 2 inputs, the input vector, x_t and the previous hidden state, h_{t-1} of the GRU and outputs,\n",
    "    # an output vector, y_t (output) and the current hidden state, h_t (state) of the GRU.\n",
    "\n",
    "    # We feed the embedded decoder input concatenated with the context vector from our attention model into the GRU.\n",
    "\n",
    "    # context vector before expansion: (batch_size, hidden_size)\n",
    "    # context vector after expansion: (batch_size, 1, hidden_size)\n",
    "    # x: (batch_size, 1, embedding_dim)\n",
    "    # x concatenated with context vector: (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "    if print_stuff:\n",
    "      print(f'Shape of concatenated expanded context vector with embedded decoder input (RNN): {x.shape}.')\n",
    "\n",
    "    # passing the concatenated vector above to the GRU generates 2 outputs: output and state\n",
    "    # shape of x before GRU: (batch_size, 1, embedding_dim + hidden_size)\n",
    "    # shape of output: (batch_size, max_length, ?)\n",
    "    # shape of state: (batch_size hidden_size = units)\n",
    "    output, state = self.gru(x)\n",
    "    if print_stuff:\n",
    "      print(f'Shape of output of GRU (RNN): {output.shape}.')\n",
    "      print(f'Shape of state of GRU (RNN): {state.shape}.')\n",
    "\n",
    "    # output before fc1: (batch_size, max_length, ?)\n",
    "    # output after fc1: (batch_size, max_length, hidden_size = units)\n",
    "    x = self.fc1(output)\n",
    "    if print_stuff:\n",
    "      print(f'Shape of output of GRU after fc1 (RNN): {x.shape}.')\n",
    "\n",
    "    # x before reshape: (batch_size, max_length, hidden_size = units)\n",
    "    # x after reshape: (batch_size * max_length, hidden_size = units)\n",
    "    # we force there to be (units) columns, and let TensorFlow take care of finding out the\n",
    "    # number of rows needed in order to preserve the area of our tensor by inputting -1.\n",
    "    x = tf.reshape(x, (-1, x.shape[2]))\n",
    "    if print_stuff:\n",
    "      print(f'Shape after reshaping (RNN): {x.shape}.')\n",
    "\n",
    "    # x before fc2: (batch_size * max_length, hidden_size = units)\n",
    "    # x after fc2: (batch_size * max_length, vocab_size)\n",
    "    x = self.fc2(x)\n",
    "    if print_stuff:\n",
    "      print(f'Shape after fc2 (RNN): {x.shape}.')\n",
    "\n",
    "    # x: model's prediction\n",
    "    # hidden: model's hidden state\n",
    "    # attention_weights: weights from attention\n",
    "    if print_stuff:\n",
    "      print(f'Shape of prediction (RNN): {x.shape}')\n",
    "      print(f'Shape of hidden state (RNN): {state.shape}')\n",
    "      print(f'Shape of attention weights (RNN): {attention_weights.shape}')\n",
    "    return x, state, attention_weights\n",
    "\n",
    "  # This function below unveils to us that the value of hidden_size is equal to units\n",
    "  \n",
    "  def reset_state(self, batch_size):\n",
    "    return tf.zeros((batch_size, self.units))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We examine the print statements which shed light on the dimensionality of the various components of our model.\n",
    "# embedding_dim = 256\n",
    "# units = 512\n",
    "# top_k = 5000\n",
    "# vocab_size = top_k + 1\n",
    "\n",
    "# encoder = CNN_Encoder(embedding_dim)\n",
    "# decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
    "\n",
    "# for (batch, (img_tensor, target)) in enumerate(dataset_val_test):\n",
    "\n",
    "#     dec_input = tf.expand_dims([tokenizer_val[1].word_index['<start>']] * target.shape[0], 1)\n",
    "\n",
    "#     features = encoder(img_tensor, True)\n",
    "#     print(f'Shape of features (CNN): {features.shape}.')\n",
    "\n",
    "#     hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "#     print(f'Shape of hidden generated from reset_state method (RNN): {hidden.shape}.')\n",
    "    \n",
    "#     decoder(dec_input, features, hidden, True)\n",
    "#     break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Creating an instance of the encoder and decoder models.\n",
    "\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "top_k = 5000\n",
    "vocab_size = top_k + 1\n",
    "\n",
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optimizer = tf.keras.optimizers.Adam() # choosing a variant of stochastic gradient descent\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "    # This is ultimately a classification problem, where we wish to predict with high probability the next sensible word in our\n",
    "    # caption, and a low probability for all other words. We have not one-hot encoded our words, so we do not use categorical\n",
    "    # cross entropy. We have used integer labels instead, so we use sparse categorical cross entropy.\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0)) # a mask that reflects if real is not equal to 0\n",
    "  loss_ = loss_object(real, pred) # calculating the loss relative to real and pred\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype) # promoting mask to a tensor with the dtype of loss_\n",
    "  loss_ *= mask # The mask is full of 1's and 0's and the categorical cross entropy is a product of 1's and 0's with\n",
    "  # the logarithm of a softmax probability.\n",
    "\n",
    "  return tf.reduce_mean(loss_) # computing the mean across all dimensions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Setting up checkpoints.\n",
    "checkpoint_path = \"./checkpoints/train\"\n",
    "# Keeping track of trackable variables, such as the encoder, decoder, optimizer.\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder, decoder=decoder, optimizer=optimizer) \n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5) # We only allow the checkpoint manager\n",
    "# to keep 5 checkpoints at most."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "  # restoring the latest checkpoint in checkpoint_path\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "loss_plot = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We use the tokenizer for the training dataset at every training step, so we run it now.\n",
    "# We also create the training dataset as we will use multiple times during training\n",
    "# tokenizer_train = tokenize_pad(captions_and_images('train')[0])[1]\n",
    "# dataset_train = create_dataset('train', 64)\n",
    "# print(len(pro_img_and_tok_cap('train')[0]))\n",
    "\n",
    "tokenizer_train = tokenize_pad(captions_and_images('val')[0]) # referred to as tokenizer_train, but originates from\n",
    "# the validation dataset.\n",
    "dataset_train = create_dataset('val') # referred to as dataset_train, but originates from the validation dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "  loss = 0\n",
    "  # initializing the hidden state for each batch because the captions are not related from image to image\n",
    "  hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "  # The decoder input are the tokenized and padded captions. We initiate (batch_size) inputs to the start token\n",
    "  # and an extra dimension to take into which step or time frame we are at.\n",
    "  dec_input = tf.expand_dims([tokenizer_train[1].word_index['<start>']] * target.shape[0], 1)\n",
    "\n",
    "  with tf.GradientTape() as tape: # used to keep track of certain variables, so we can efficiently carry out\n",
    "    # backpropagation\n",
    "      features = encoder(img_tensor, False) # The extracted features output of ResNet-50.\n",
    "\n",
    "      for i in range(1, target.shape[1]):\n",
    "          # target.shape[1] are the number maximum number of words any tokenized and padded caption can have.\n",
    "          # We now pass the decoder input\n",
    "          predictions, hidden, _ = decoder(dec_input, features, hidden, False)\n",
    "\n",
    "          # The loss is given by comparing the ith word in our caption (entire batch selected with colon) and our predictions\n",
    "          loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "          # Using teacher forcing means that the next decoder input is given by the word our model should have predicted.\n",
    "          dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "  total_loss = (loss / int(target.shape[1]))\n",
    "  \n",
    "  # Here, we can appreciate why we wanted to inherit attributes and methods from tf.keras.Model.\n",
    "  # tf.keras.Model inherits from tf.Module and tf.Module has an attribute called trainable_variables, which\n",
    "  # is a collection of all the variables that are being trained.\n",
    "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, trainable_variables) # differentiate loss function with respect to all trainable variables\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, trainable_variables)) #\n",
    "\n",
    "  return loss, total_loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "EPOCHS = 13\n",
    "num_steps = len(pro_img_and_tok_cap('val')[0]) // 64\n",
    "    \n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset_train[0]): # we create the dataset beforehand to avoid generating it\n",
    "        # multiple times\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 2 ==0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n",
    "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('mlp_1_env': conda)"
  },
  "interpreter": {
   "hash": "56391efcfd407cdfb7177f7052d9ec0dabb31d2434d8abe883c176e68133b969"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}